{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup, DataCollatorWithPadding, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, Dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from fewShotModel import FewSoftModel\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch\n",
    "import os\n",
    "from datasets import DatasetDict, Dataset, load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "print(\"Init model and tokenizer\")\n",
    "path = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, token=\"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\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token\n",
    "\n",
    "# login()\n",
    "\n",
    "LLM_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=path,\n",
    "    device_map='auto',\n",
    "    cache_dir = \"./llama13b\",\n",
    "    token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Init dataset\")\n",
    "task = \"piqa\"\n",
    "num_shots = 3\n",
    "dataset = load_from_disk(f'datasets/FewSoftPrompting/{task}/{num_shots}shot')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Init PEFT\")\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,\n",
    "    # prompt_tuning_init_text=f\"{INNIT_DICT_FEW_SHOT[self.task]}\",\n",
    "    num_virtual_tokens= 8,\n",
    "    # self.num_virtual_tokens,\n",
    "    tokenizer_name_or_path=path\n",
    ")\n",
    "PEFT_model = get_peft_model(model=LLM_model, peft_config=peft_config)\n",
    "PEFT_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=0.0035,\n",
    "    num_train_epochs=8\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=PEFT_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(\"outputs\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
